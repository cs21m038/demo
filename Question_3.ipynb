{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs21m038/demo/blob/master/Question_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ugd5UH7kcbrS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import fashion_mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "163Zh--_y600"
      },
      "source": [
        "Fetching data from fashion mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV9AknlvPrBp",
        "outputId": "40b39086-c475-4976-995e-7116879cdf29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_train_val, y_train, y_train_val = train_test_split(x_train, y_train, test_size=0.10, random_state=0)\n",
        "\n",
        "# x_train = np.array(x_train, dtype='float128').reshape([len(x_train), 784])[:1000].T\n",
        "# y_train = np.array(y_train, dtype='float128')[:1000]\n",
        "# x_test = np.array(x_test, dtype='float128').reshape([len(x_test), 784])[:100].T\n",
        "# y_test = np.array(y_test, dtype='float128')[:100]\n",
        "\n",
        "x_train = np.array(x_train, dtype='longdouble').reshape([len(x_train), 784]).T\n",
        "y_train = np.array(y_train)\n",
        "x_train_val = np.array(x_train_val, dtype='longdouble').reshape([len(x_train_val), 784]).T\n",
        "y_train_val = np.array(y_train_val)\n",
        "x_test = np.array(x_test, dtype='longdouble').reshape([len(x_test), 784]).T\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "x_train = x_train/255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUnpfH6iZ3D2",
        "outputId": "75cae169-e31d-4e6b-b490-8de60e8912a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "(y_train.shape)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KagKqsJczD04"
      },
      "source": [
        "one hot vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qD1JmUj5PrDt"
      },
      "outputs": [],
      "source": [
        "def one_hot(Y):\n",
        "  one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
        "  one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "  return one_hot_Y.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOyp0OJDzHlq"
      },
      "source": [
        "activation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Z3_G7mmzPrFa"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1 + np.exp(-x))\n",
        "\n",
        "def d_sigmoid(x):\n",
        "  return (1 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(x, 0)\n",
        "\n",
        "def d_relu(x):\n",
        "  return x > 0\n",
        "\n",
        "def tanh(x):\n",
        "  return (2/(1+np.exp(-2*x)))-1\n",
        "\n",
        "def d_tanh(x):\n",
        "  return 1-tanh(x)**2\n",
        "\n",
        "# def softmax(Z):\n",
        "#   A = np.exp(Z) / sum(np.exp(Z))\n",
        "#   return A\n",
        "def softmax(x):\n",
        "    e = np.exp(x - np.max(x))\n",
        "    if e.ndim == 1:\n",
        "        return e / np.sum(e, axis=0)\n",
        "    else: # dim = 2\n",
        "        return e / np.sum(e, axis=1, keepdims=True)\n",
        "\n",
        "def entropy(y,a):\n",
        "  return -(y*np.log(a) + (1-y)*np.log(1-a))\n",
        "\n",
        "def d_entropy(y,a):\n",
        "  return (a - y)\n",
        "  #/(a*(1 - a))\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvJFkeCmzGiK"
      },
      "source": [
        "implementing AddLayer class to add layers to the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Necip8DFPrHI"
      },
      "outputs": [],
      "source": [
        "class AddLayer:\n",
        "  def __init__(self, input_size, layer_size, act_fn,itype):\n",
        "    if itype == \"random\":\n",
        "      self.w = np.random.randn(layer_size, input_size)\n",
        "      self.b = np.random.randn(layer_size,1)\n",
        "      self.w = self.w.astype('longdouble')\n",
        "      self.w = self.w.astype('longdouble')\n",
        "    else:\n",
        "      self.w = np.random.randn(layer_size, input_size)* np.sqrt(1/input_size)\n",
        "      self.b = np.random.randn(layer_size,1)* np.sqrt(1/input_size)\n",
        "      self.w = self.w.astype('longdouble')\n",
        "      self.w = self.w.astype('longdouble')\n",
        "\n",
        "    self.dW = np.zeros((layer_size, input_size))\n",
        "    self.db = np.zeros((layer_size, 1))\n",
        "    self.act = get_activation_func(act_fn)\n",
        "    if(act_fn != \"softmax\"): self.d_act = get_d_activation_func(act_fn)\n",
        "\n",
        "    self.prev_v_w, self.prev_v_b, self.gamma = 0, 0, 0.9\n",
        "    self.m_w, self.m_b = 0, 0\n",
        "\n",
        "  def feedForward(self, inputs):\n",
        "    self.h_prev = inputs\n",
        "    self.a = self.w.dot(self.h_prev) + self.b\n",
        "    self.h = self.act(self.a)\n",
        "    return self.h\n",
        "\n",
        "  def set_dA(self, dA):\n",
        "    self.dA = dA\n",
        "  \n",
        "  def refresh_dWb(self):\n",
        "    self.dW = np.zeros(self.dW.shape)\n",
        "    self.db = np.zeros(self.db.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kZZOpXCzh-Z"
      },
      "source": [
        "Adding layers to the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oWu4YuubPrON"
      },
      "outputs": [],
      "source": [
        "def init_network(hidden_layers_list, act_fn, init_type):\n",
        "  n_hidden_layers = len(hidden_layers_list)\n",
        "  layers = []\n",
        "  inputs = x_train\n",
        "  for i in range(n_hidden_layers):\n",
        "    #print(i , \" \", inputs.shape)\n",
        "    layer = AddLayer(len(inputs), hidden_layers_list[i], act_fn, init_type)\n",
        "    layer.feedForward(inputs)\n",
        "    inputs = layer.h\n",
        "    layers.append(layer)\n",
        "\n",
        "  outputLayer = AddLayer(len(layers[-1].h), 10, \"softmax\", init_type)\n",
        "  output = outputLayer.feedForward(layers[-1].h)\n",
        "  layers.append(outputLayer)\n",
        "\n",
        "  return layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygLyEBUOzmFi"
      },
      "source": [
        "forward and backward propagation functions and some utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Km4dzEOHvh9e"
      },
      "outputs": [],
      "source": [
        "def compute_gradient(layers, curLayer, ey, h, ind, batch_indices):\n",
        "  nxtLayer = curLayer + 1\n",
        "  # start = batch_indices[ind - 1]\n",
        "  # end = batch_indices[ind]\n",
        "  dA = []\n",
        "\n",
        "  # for i in range(start,end):\n",
        "  if curLayer == -1:\n",
        "    dA.append(-(ey[:,ind] - h[:,ind]))\n",
        "  else:\n",
        "    dA.append((layers[nxtLayer].w.T).dot(layers[nxtLayer].dA[:,ind]) * layers[curLayer].d_act((layers[curLayer].a[:,ind])))\n",
        "  return dA\n",
        "\n",
        "\n",
        "def get_activation_func(ac_func):\n",
        "  ac_dict = {\n",
        "      \"relu\" : relu,\n",
        "      \"sigmoid\" : sigmoid,\n",
        "      \"tanh\" : tanh,\n",
        "      \"softmax\" : softmax\n",
        "  }\n",
        "  return ac_dict[ac_func]\n",
        "\n",
        "def get_d_activation_func(ac_func):\n",
        "  d_ac_dict = {\n",
        "      \"relu\" : d_relu,\n",
        "      \"sigmoid\" : d_sigmoid,\n",
        "      \"tanh\" : d_tanh\n",
        "  }\n",
        "  return d_ac_dict[ac_func]\n",
        "\n",
        "\n",
        "def get_gd(gd):\n",
        "  gd_dict = {\n",
        "      \"vanilla\" : vanilla_gd,\n",
        "      \"momentum\" : momentum_gd,\n",
        "      \"nesterov\" : nesterov_gd,\n",
        "      \"stochastic\" : stochastic_gd,\n",
        "      \"rmsprop\" : rmsprop_gd,\n",
        "      \"adam\" : adam_gd,\n",
        "      \"nadam\" : nadam_gd\n",
        "  }\n",
        "  return gd_dict[gd]\n",
        "\n",
        "\n",
        "def get_batch_indices(batch_size):\n",
        "  m = (x_train.shape)[1]\n",
        "  batch_indices = np.arange(0, m, batch_size)\n",
        "  batch_indices = np.append(batch_indices, m)\n",
        "  return batch_indices\n",
        "\n",
        "\n",
        "def forward_propagation(layers, h):\n",
        "  for layer in layers:\n",
        "        h = layer.feedForward(h)\n",
        "  return h\n",
        "\n",
        "def back_propagation(layers, ey, h, gd, learning_rate, update):\n",
        "#   if(gd == \"vanilla\" and b_sz < (y_train.shape)[0]):\n",
        "#     b_sz = (y_train.shape)[0]\n",
        "\n",
        "    grad_descent = get_gd(gd)\n",
        "    outputLayer = -1\n",
        "    dA = -(ey - h.flatten())\n",
        "    dA = np.array([dA]).T\n",
        "#     print(\"ey, h, dA\",ey.shape, h.shape, dA.shape)\n",
        "#   layers[outputLayer].set_dA(dA)\n",
        "#   grad_descent(layers, outputLayer, ey, h, learning_rate, update)\n",
        "    for curLayer in range((len(layers)-1), 0, -1):\n",
        "        dA = grad_descent(layers, curLayer, ey, h, learning_rate, dA, update)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1kM8gOevuWa"
      },
      "source": [
        "#gradient descent functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jJPP-I6iPrQC"
      },
      "outputs": [],
      "source": [
        "def vanilla_gd(layers, curLayer, ey, h, b_sz, learning_rate):\n",
        "  b_arr = get_batch_indices(b_sz)\n",
        "  dA = compute_gradient(layers, curLayer, ey, h, 1, b_arr)\n",
        "  dA = np.array(dA).T\n",
        "  layers[curLayer].set_dA(dA)\n",
        "\n",
        "  layers[curLayer].dW = 1/layers[curLayer].dA.shape[1] * layers[curLayer].dA.dot(layers[curLayer].h_prev.T)\n",
        "  layers[curLayer].db = 1/layers[curLayer].dA.shape[1] * np.sum(layers[curLayer].dA)\n",
        "\n",
        "  layers[curLayer].w = layers[curLayer].w - learning_rate * layers[curLayer].dW\n",
        "  layers[curLayer].b = layers[curLayer].b - learning_rate * layers[curLayer].db\n",
        "\n",
        "\n",
        "\n",
        "def momentum_gd(layers, curLayer, ey, h, b_sz, learning_rate):\n",
        "  final_dA = []\n",
        "  b_arr = get_batch_indices(b_sz)\n",
        "\n",
        "  for i in range(1,len(b_arr)):\n",
        "    start = b_arr[i-1]\n",
        "    end = b_arr[i]\n",
        "    for j in range(start, end):\n",
        "      #feeding with new w_lookahead and b_lookahead to compute new dA\n",
        "      h_previ = np.array([layers[curLayer].h_prev[:, j]]).T\n",
        "      layers[curLayer].a[:, j] = (layers[curLayer].w.dot(h_previ) + layers[curLayer].b).flatten()\n",
        "      layers[curLayer].h[:, j] = layers[curLayer].act(layers[curLayer].a[:, j])\n",
        "\n",
        "      dA = compute_gradient(layers, curLayer, ey, h, j, b_arr)\n",
        "      for arr in dA:\n",
        "        final_dA.append(arr)\n",
        "      dA = np.array(dA).T\n",
        "      \n",
        "      layers[curLayer].dW += dA.dot(h_previ.T)\n",
        "      layers[curLayer].db += np.sum(dA)\n",
        "\n",
        "    v_w = layers[curLayer].gamma*layers[curLayer].prev_v_w + learning_rate*layers[curLayer].dW\n",
        "    v_b = layers[curLayer].gamma*layers[curLayer].prev_v_b + learning_rate*layers[curLayer].db\n",
        "    layers[curLayer].refresh_dWb()\n",
        "\n",
        "    layers[curLayer].w = layers[curLayer].w - v_w\n",
        "    layers[curLayer].b = layers[curLayer].b - v_b\n",
        "\n",
        "    layers[curLayer].prev_v_w = v_w\n",
        "    layers[curLayer].prev_v_b = v_b\n",
        "\n",
        "  final_dA = np.array(final_dA).T\n",
        "  layers[curLayer].set_dA(final_dA)\n",
        "\n",
        "\n",
        "\n",
        "def nesterov_gd(layers, curLayer, ey, h, b_sz, learning_rate):\n",
        "  final_dA = []\n",
        "  b_arr = get_batch_indices(b_sz)\n",
        "\n",
        "  for i in range(1,len(b_arr)):\n",
        "    start = b_arr[i-1]\n",
        "    end = b_arr[i]\n",
        "\n",
        "    v_w = layers[curLayer].gamma*layers[curLayer].prev_v_w\n",
        "    v_b = layers[curLayer].gamma*layers[curLayer].prev_v_b\n",
        "\n",
        "    w_lookAhead = layers[curLayer].w - v_w\n",
        "    b_lookAhead = layers[curLayer].b - v_b\n",
        "\n",
        "    layers[curLayer].w = w_lookAhead\n",
        "    layers[curLayer].b = b_lookAhead\n",
        "\n",
        "    for j in range(start, end):\n",
        "      #feeding with new w_lookahead and b_lookahead to compute new dA\n",
        "      h_previ = np.array([layers[curLayer].h_prev[:, j]]).T\n",
        "      layers[curLayer].a[:, j] = (layers[curLayer].w.dot(h_previ) + layers[curLayer].b).flatten()\n",
        "      layers[curLayer].h[:, j] = layers[curLayer].act(layers[curLayer].a[:, j])\n",
        "\n",
        "      dA = compute_gradient(layers, curLayer, ey, h, j, b_arr)\n",
        "      for arr in dA:\n",
        "        final_dA.append(arr)\n",
        "      dA = np.array(dA).T\n",
        "      \n",
        "      layers[curLayer].dW += dA.dot(h_previ.T)\n",
        "      layers[curLayer].db += np.sum(dA)\n",
        "\n",
        "    v_w = layers[curLayer].gamma*layers[curLayer].prev_v_w + learning_rate*layers[curLayer].dW\n",
        "    v_b = layers[curLayer].gamma*layers[curLayer].prev_v_b + learning_rate*layers[curLayer].db\n",
        "    layers[curLayer].refresh_dWb()\n",
        "    \n",
        "    layers[curLayer].w = layers[curLayer].w - v_w\n",
        "    layers[curLayer].b = layers[curLayer].b - v_b\n",
        "\n",
        "    layers[curLayer].prev_v_w = v_w\n",
        "    layers[curLayer].prev_v_b = v_b\n",
        "\n",
        "  final_dA = np.array(final_dA).T\n",
        "  layers[curLayer].set_dA(final_dA)\n",
        "\n",
        "\n",
        "\n",
        "def stochastic_gd(layers, curLayer, ey, h, learning_rate, dA, update):\n",
        "    h_previ = np.array(layers[curLayer].h_prev)\n",
        "#     layers[curLayer].a = (layers[curLayer].w.dot(h_previ) + layers[curLayer].b).flatten()\n",
        "#     layers[curLayer].h = layers[curLayer].act(layers[curLayer].a)\n",
        "    \n",
        "    layers[curLayer].dW += dA.dot(h_previ.T)\n",
        "    layers[curLayer].db += dA\n",
        "    \n",
        "#     print(\"h_prev of cur\",layers[curLayer].h_prev.shape)\n",
        "#     print(\"dA next\", dA.shape)\n",
        "    dh = (layers[curLayer].w.T).dot(dA)\n",
        "    dA = dh * layers[curLayer-1].d_act((layers[curLayer-1].a))\n",
        "#     print(\"dWcur\",layers[curLayer].dW.shape)\n",
        "    if update != 0:\n",
        "        layers[curLayer].w = layers[curLayer].w - learning_rate * layers[curLayer].dW\n",
        "        layers[curLayer].b = layers[curLayer].b - learning_rate * layers[curLayer].db\n",
        "        layers[curLayer].refresh_dWb()\n",
        "\n",
        "#   final_dA = np.array(final_dA).T\n",
        "#     layers[curLayer].set_dA(dA)\n",
        "    return dA\n",
        "\n",
        "\n",
        "def rmsprop_gd(layers, curLayer, ey, h, b_sz, eta):\n",
        "  final_dA = []\n",
        "  b_arr = get_batch_indices(b_sz)\n",
        "  beta = 0.9\n",
        "  eps = 1e-8\n",
        "\n",
        "  for i in range(1,len(b_arr)):\n",
        "    start = b_arr[i-1]\n",
        "    end = b_arr[i]\n",
        "    for j in range(start, end):\n",
        "      #feeding with new w_lookahead and b_lookahead to compute new dA\n",
        "      h_previ = np.array([layers[curLayer].h_prev[:, j]]).T\n",
        "      layers[curLayer].a[:, j] = (layers[curLayer].w.dot(h_previ) + layers[curLayer].b).flatten()\n",
        "      layers[curLayer].h[:, j] = layers[curLayer].act(layers[curLayer].a[:, j])\n",
        "\n",
        "      dA = compute_gradient(layers, curLayer, ey, h, j, b_arr)\n",
        "      for arr in dA:\n",
        "        final_dA.append(arr)\n",
        "      dA = np.array(dA).T\n",
        "      \n",
        "      layers[curLayer].dW += dA.dot(h_previ.T)\n",
        "      layers[curLayer].db += np.sum(dA)\n",
        "\n",
        "    v_w = beta*layers[curLayer].prev_v_w + (1 - beta)*(layers[curLayer].dW**2)\n",
        "    v_b = beta*layers[curLayer].prev_v_b + beta*(layers[curLayer].db**2)\n",
        "\n",
        "\n",
        "    layers[curLayer].w = layers[curLayer].w - (eta/(np.sqrt(v_w + eps)))*layers[curLayer].dW\n",
        "    layers[curLayer].b = layers[curLayer].b - (eta/(np.sqrt(v_b + eps)))*layers[curLayer].db\n",
        "    layers[curLayer].refresh_dWb()\n",
        "    \n",
        "    layers[curLayer].prev_v_w = v_w\n",
        "    layers[curLayer].prev_v_b = v_b\n",
        "\n",
        "  final_dA = np.array(final_dA).T\n",
        "  layers[curLayer].set_dA(final_dA)\n",
        "\n",
        "\n",
        "def adam_gd(layers, curLayer, ey, h, b_sz, eta):\n",
        "  final_dA = []\n",
        "  b_arr = get_batch_indices(b_sz)\n",
        "  beta1, beta2, eps, t = 0.9, 0.99, 1e-8, 0\n",
        "\n",
        "  for i in range(1,len(b_arr)):\n",
        "    start = b_arr[i-1]\n",
        "    end = b_arr[i]\n",
        "    for j in range(start, end):\n",
        "      #feeding with new w_lookahead and b_lookahead to compute new dA\n",
        "      h_previ = np.array([layers[curLayer].h_prev[:, j]]).T\n",
        "      layers[curLayer].a[:, j] = (layers[curLayer].w.dot(h_previ) + layers[curLayer].b).flatten()\n",
        "      layers[curLayer].h[:, j] = layers[curLayer].act(layers[curLayer].a[:, j])\n",
        "\n",
        "      dA = compute_gradient(layers, curLayer, ey, h, j, b_arr)\n",
        "      for arr in dA:\n",
        "        final_dA.append(arr)\n",
        "      dA = np.array(dA).T\n",
        "      \n",
        "      layers[curLayer].dW += dA.dot(h_previ.T)\n",
        "      layers[curLayer].db += np.sum(dA)\n",
        "\n",
        "    m_w = beta1*layers[curLayer].m_w + (1 - beta1)*(layers[curLayer].dW)\n",
        "    m_b = beta2*layers[curLayer].m_b + (1 - beta1)*(layers[curLayer].db)\n",
        "\n",
        "    v_w = beta2*layers[curLayer].prev_v_w + (1 - beta2)*(layers[curLayer].dW**2)\n",
        "    v_b = beta2*layers[curLayer].prev_v_b + (1 - beta2)*(layers[curLayer].db**2)\n",
        "\n",
        "    t = t + 1\n",
        "    m_w_hat = m_w/(1-np.power(beta1,t))\n",
        "    m_b_hat = m_b/(1-np.power(beta1,t))\n",
        "\n",
        "    v_w_hat = v_w/(1-np.power(beta2,t))\n",
        "    v_b_hat = v_b/(1-np.power(beta2,t))\n",
        "\n",
        "    #updates \n",
        "    layers[curLayer].w = layers[curLayer].w - (eta/(np.sqrt(v_w_hat + eps)))*m_w_hat\n",
        "    layers[curLayer].b = layers[curLayer].b - (eta/(np.sqrt(v_b_hat + eps)))*m_b_hat\n",
        "    layers[curLayer].refresh_dWb()\n",
        "\n",
        "    layers[curLayer].prev_v_w = v_w\n",
        "    layers[curLayer].prev_v_b = v_b\n",
        "\n",
        "    layers[curLayer].m_w = m_w\n",
        "    layers[curLayer].m_b = m_b\n",
        "\n",
        "  final_dA = np.array(final_dA).T\n",
        "  layers[curLayer].set_dA(final_dA)\n",
        "\n",
        "\n",
        "def nadam_gd(layers, curLayer, ey, h, b_sz, eta):\n",
        "  final_dA = []\n",
        "  b_arr = get_batch_indices(b_sz)\n",
        "  beta1, beta2, eps, t = 0.9, 0.99, 1e-8, 0\n",
        "\n",
        "  for i in range(1,len(b_arr)):\n",
        "    start = b_arr[i-1]\n",
        "    end = b_arr[i]\n",
        "    for j in range(start, end):\n",
        "      #feeding with new w_lookahead and b_lookahead to compute new dA\n",
        "      h_previ = np.array([layers[curLayer].h_prev[:, j]]).T\n",
        "      layers[curLayer].a[:, j] = (layers[curLayer].w.dot(h_previ) + layers[curLayer].b).flatten()\n",
        "      layers[curLayer].h[:, j] = layers[curLayer].act(layers[curLayer].a[:, j])\n",
        "\n",
        "      dA = compute_gradient(layers, curLayer, ey, h, j, b_arr)\n",
        "      for arr in dA:\n",
        "        final_dA.append(arr)\n",
        "      dA = np.array(dA).T\n",
        "      \n",
        "      layers[curLayer].dW += dA.dot(h_previ.T)\n",
        "      layers[curLayer].db += np.sum(dA)\n",
        "\n",
        "    m_w = beta1*layers[curLayer].m_w + (1 - beta1)*(layers[curLayer].dW)\n",
        "    m_b = beta2*layers[curLayer].m_b + (1 - beta1)*(layers[curLayer].db)\n",
        "\n",
        "    v_w = beta2*layers[curLayer].prev_v_w + (1 - beta2)*(layers[curLayer].dW**2)\n",
        "    v_b = beta2*layers[curLayer].prev_v_b + (1 - beta2)*(layers[curLayer].db**2)\n",
        "\n",
        "    t = t + 1\n",
        "    m_w_hat = m_w/(1-np.power(beta1,t))\n",
        "    m_b_hat = m_b/(1-np.power(beta1,t))\n",
        "\n",
        "    m_w_lookahead = beta1*m_w_hat - ((1 - beta1)*layers[curLayer].dW/(1-np.power(beta1,t)))\n",
        "    m_b_lookahead = beta1*m_b_hat - ((1 - beta1)*layers[curLayer].db/(1-np.power(beta1,t)))\n",
        "\n",
        "    v_w_hat = v_w/(1-np.power(beta2,t))\n",
        "    v_b_hat = v_b/(1-np.power(beta2,t))\n",
        "\n",
        "    #updates \n",
        "    layers[curLayer].w = layers[curLayer].w - (eta/(np.sqrt(v_w_hat + eps)))*m_w_lookahead\n",
        "    layers[curLayer].b = layers[curLayer].b - (eta/(np.sqrt(v_b_hat + eps)))*m_b_lookahead\n",
        "    layers[curLayer].refresh_dWb()\n",
        "    \n",
        "    layers[curLayer].prev_v_w = v_w\n",
        "    layers[curLayer].prev_v_b = v_b\n",
        "\n",
        "    layers[curLayer].m_w = m_w\n",
        "    layers[curLayer].m_b = m_b\n",
        "\n",
        "  final_dA = np.array(final_dA).T\n",
        "  layers[curLayer].set_dA(final_dA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "HSIJoblOEIqT"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# x = np.array([[2,3,4,5,1],[6,7,8,9,10],[21,22,23,24,25]])\n",
        "\n",
        "# dotx = []\n",
        "# for i in range(x.shape[1]):\n",
        "#   dotx.append(x[:,i])\n",
        "#   print(dotx)\n",
        "# dotx = np.array(dotx).T\n",
        "# print(dotx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTHsc7arzlUS"
      },
      "source": [
        "measuring functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "G_SJIdWkPrKo"
      },
      "outputs": [],
      "source": [
        "def get_predictions(A2):\n",
        "    return np.argmax(A2, 0)\n",
        "def get_accuracy(predictions, Y):\n",
        "    return np.sum(predictions == Y) / Y.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCuJU6SozwBm"
      },
      "source": [
        "# RUNNING EPOCHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HF_ebtpPPrRr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "d64b968f-d0cf-4427-fb70-e2de962e00e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-72a1de86b60f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0my_cap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mback_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stochastic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#     ey = one_hot(y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-4d6fe4b90730>\u001b[0m in \u001b[0;36mback_propagation\u001b[0;34m(layers, ey, h, gd, learning_rate, update)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m#   grad_descent(layers, outputLayer, ey, h, learning_rate, update)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcurLayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mdA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-6e3523ac6cf7>\u001b[0m in \u001b[0;36mstochastic_gd\u001b[0;34m(layers, curLayer, ey, h, learning_rate, dA, update)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m#     layers[curLayer].h = layers[curLayer].act(layers[curLayer].a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurLayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdW\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_previ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurLayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "m = x_train.shape[1]\n",
        "epochs = 5 # no of iterations\n",
        "hidden_layers_list = [128,128]\n",
        "batch_size =  16\n",
        "activation = \"relu\"\n",
        "init_type = \"random\"\n",
        "layers = init_network(hidden_layers_list, activation, init_type)\n",
        "learning_rate = 0.001\n",
        "cost = []\n",
        "ey = one_hot(y_train)\n",
        "for epoch in range(epochs):\n",
        "    b_arr = get_batch_indices(batch_size)\n",
        "    for i in range(1,len(b_arr)):\n",
        "        start = b_arr[i-1]\n",
        "        end = b_arr[i]\n",
        "        for j in range(start, end):\n",
        "            x_data = np.array([x_train[:,j]]).T\n",
        "            y_cap = forward_propagation(layers, x_data)\n",
        "            flag = ((j == start and j != 0) or j+1 == m)\n",
        "            back_propagation(layers, ey[:,j], y_cap, \"stochastic\", learning_rate, flag)\n",
        "\n",
        "#     ey = one_hot(y_train)\n",
        "    loss = 1/m * np.sum(entropy(ey, y_cap)) #this h is y_hat now \n",
        "    cost.append(loss)\n",
        "\n",
        "    if epoch % 1 == 0 or epoch == epochs-1:\n",
        "        y_cap = forward_propagation(layers, x_train)\n",
        "        print(\"Iteration: \", epoch, \"\\n\")\n",
        "        print(\"accuracy on train data = \", get_accuracy(get_predictions(y_cap), y_train))\n",
        "\n",
        "        y_test_pred = x_train_val\n",
        "        for layer in layers:\n",
        "            y_test_pred = layer.feedForward(y_test_pred)\n",
        "        print(\"accuracy on validation  = \", get_accuracy(get_predictions(y_test_pred), y_train_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m75I2m0uz6_3"
      },
      "source": [
        "# TESTING DATA WITH IMAGE SHOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVV4VhLiPrWv"
      },
      "outputs": [],
      "source": [
        "y_test_pred = x_test\n",
        "for layer in layers:\n",
        "  y_test_pred = layer.feedForward(y_test_pred)\n",
        "\n",
        "print(\"accuracy on test data : \", get_accuracy(get_predictions(y_test_pred), y_test))\n",
        "\n",
        "def test_prediction(index):\n",
        "  current_image = x_train[:, index, None]\n",
        "  prediction = np.argmax(y_test_pred[:,index], 0)\n",
        "  label = y_train[index]\n",
        "  print(y_test_pred[:,index])\n",
        "  print(\"Prediction: \", prediction)\n",
        "  print(\"Label: \", label)\n",
        "  \n",
        "  current_image = current_image.reshape((28, 28)) * 255\n",
        "  plt.gray()\n",
        "  plt.imshow(current_image, interpolation='nearest')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWo7QwSTPraL"
      },
      "outputs": [],
      "source": [
        "test_prediction(7)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Question_3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}